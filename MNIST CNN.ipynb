{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/mnist_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1255f7da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 201 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[0.09483402 0.11678492 0.08456551 0.09778703 0.09051102 0.10311837\n",
      "  0.10322042 0.10068547 0.10331434 0.10517888]\n",
      " [0.09843902 0.09929983 0.08888844 0.09507325 0.09659586 0.10816114\n",
      "  0.10414679 0.0980856  0.10426011 0.10705002]\n",
      " [0.09811319 0.10729353 0.08795496 0.10043881 0.0980774  0.10843193\n",
      "  0.10934542 0.09593308 0.09659989 0.09781174]\n",
      " [0.09996164 0.09888086 0.08873963 0.09539348 0.0952784  0.10135579\n",
      "  0.09556668 0.11369529 0.09842976 0.11269841]\n",
      " [0.08779081 0.09644245 0.097711   0.09683439 0.10028975 0.10082298\n",
      "  0.10708194 0.09564304 0.10758211 0.10980152]\n",
      " [0.09091226 0.09967973 0.0904323  0.09432518 0.09562595 0.10764167\n",
      "  0.1033266  0.10354587 0.10019322 0.11431717]\n",
      " [0.0992707  0.08819511 0.09330061 0.10396204 0.09981562 0.10238823\n",
      "  0.10441435 0.10444062 0.10051444 0.10369828]\n",
      " [0.09759029 0.0983292  0.08844271 0.10117692 0.10581455 0.09808887\n",
      "  0.10709509 0.08907841 0.10728178 0.10710222]\n",
      " [0.08902008 0.10690886 0.09624862 0.09771685 0.09181128 0.10619041\n",
      "  0.10760031 0.09393914 0.10833019 0.10223434]\n",
      " [0.09734216 0.09145259 0.08902266 0.0883301  0.09387773 0.10679715\n",
      "  0.10846519 0.11590135 0.10600372 0.10280732]\n",
      " [0.10515074 0.09989654 0.08760156 0.10089783 0.11172897 0.08663618\n",
      "  0.09458422 0.10658717 0.10761013 0.09930661]\n",
      " [0.09776423 0.10169687 0.09544413 0.09830622 0.09877179 0.09680565\n",
      "  0.10668869 0.10182601 0.09929569 0.10340067]\n",
      " [0.09258874 0.1092069  0.08911724 0.10617446 0.10715683 0.09338436\n",
      "  0.10979981 0.09888662 0.09645263 0.09723241]\n",
      " [0.09461149 0.1026773  0.0927994  0.11545575 0.09480318 0.10126782\n",
      "  0.09388993 0.09976136 0.10500833 0.09972543]\n",
      " [0.09799011 0.11141478 0.09596714 0.09208442 0.09327348 0.10067758\n",
      "  0.10175914 0.10333035 0.1062787  0.09722431]\n",
      " [0.10208642 0.09588171 0.09402232 0.09844249 0.11087595 0.09643845\n",
      "  0.0976927  0.09961945 0.09863943 0.10630107]\n",
      " [0.10187746 0.09274256 0.10579977 0.09803527 0.10681991 0.09439004\n",
      "  0.10442635 0.10347655 0.09609884 0.09633331]\n",
      " [0.1052834  0.08622427 0.08485341 0.10086464 0.1054293  0.10112601\n",
      "  0.10590063 0.09595083 0.10659546 0.10777202]\n",
      " [0.09952017 0.09845025 0.09063616 0.0930298  0.10205207 0.1118935\n",
      "  0.10279288 0.10616329 0.09907765 0.09638421]\n",
      " [0.08471222 0.08943209 0.1000419  0.11412518 0.10570632 0.10291026\n",
      "  0.1071667  0.08988698 0.10844864 0.09756967]\n",
      " [0.09851336 0.10861174 0.09693906 0.105153   0.09082249 0.09898356\n",
      "  0.09572027 0.10163558 0.09621155 0.10740937]\n",
      " [0.09739777 0.10199852 0.09216785 0.10777698 0.09468073 0.1031789\n",
      "  0.10552929 0.10194377 0.09467846 0.10064774]\n",
      " [0.09612551 0.09571607 0.09700824 0.09967409 0.09949174 0.0938055\n",
      "  0.09967154 0.10408874 0.10469061 0.10972799]\n",
      " [0.10201427 0.10013688 0.09354494 0.09122699 0.1063358  0.10618623\n",
      "  0.09602837 0.10054932 0.10212629 0.10185089]\n",
      " [0.12075768 0.1013341  0.09997711 0.09896298 0.09022187 0.10078\n",
      "  0.0999236  0.10734739 0.09073714 0.08995802]\n",
      " [0.10370475 0.09971617 0.10695093 0.0945518  0.09707689 0.08590651\n",
      "  0.10523643 0.10217637 0.09724921 0.10743089]\n",
      " [0.10837279 0.09474646 0.07640287 0.09842893 0.1045021  0.11264437\n",
      "  0.10169803 0.10122097 0.09691176 0.10507178]\n",
      " [0.08575838 0.10178141 0.08847501 0.10895828 0.102245   0.10371252\n",
      "  0.11176648 0.0928468  0.10220712 0.102249  ]\n",
      " [0.10785885 0.09931279 0.09093996 0.10374878 0.09685506 0.1092074\n",
      "  0.10176947 0.10825035 0.08836489 0.09369244]\n",
      " [0.09415419 0.09791986 0.0936856  0.10901336 0.09868966 0.10175579\n",
      "  0.10321176 0.10160669 0.09409653 0.10586654]\n",
      " [0.09757139 0.10600027 0.09976836 0.09851753 0.09756488 0.09025372\n",
      "  0.10973176 0.10027559 0.09657925 0.10373723]\n",
      " [0.10125685 0.08959311 0.08645598 0.09922674 0.09968286 0.09381411\n",
      "  0.10603137 0.11118366 0.10814136 0.10461389]\n",
      " [0.10092039 0.10229153 0.09366108 0.095365   0.10017845 0.10067469\n",
      "  0.10404683 0.09613874 0.09654676 0.11017656]\n",
      " [0.09981268 0.09611887 0.08568352 0.0879386  0.09372503 0.10802766\n",
      "  0.10983609 0.11109085 0.10094649 0.10682018]\n",
      " [0.09469029 0.09743513 0.09591167 0.10155364 0.1022658  0.09470072\n",
      "  0.10630905 0.10138262 0.09835581 0.10739519]\n",
      " [0.09966664 0.10023583 0.0997317  0.09611997 0.09962656 0.09883267\n",
      "  0.10392801 0.09585172 0.1056461  0.10036077]\n",
      " [0.08924031 0.09883736 0.08556943 0.10459954 0.09925339 0.09902201\n",
      "  0.1068064  0.10749457 0.09984216 0.10933489]\n",
      " [0.10773412 0.08706173 0.09175807 0.10274169 0.10339246 0.10443245\n",
      "  0.10989489 0.10100109 0.09952646 0.092457  ]\n",
      " [0.09134155 0.0926741  0.09905982 0.09606388 0.10285296 0.10709265\n",
      "  0.09649345 0.10661169 0.09982947 0.10798042]\n",
      " [0.10258441 0.09854253 0.08850001 0.09151469 0.10171036 0.09572077\n",
      "  0.10056904 0.10135923 0.10901874 0.11048013]\n",
      " [0.10238767 0.09850866 0.09103839 0.10637405 0.09735709 0.09945645\n",
      "  0.10318764 0.10151848 0.10353766 0.09663399]\n",
      " [0.09301713 0.09615523 0.08874665 0.09616016 0.09950934 0.0990986\n",
      "  0.11145116 0.10586111 0.10179529 0.10820542]\n",
      " [0.11363342 0.09584155 0.09366666 0.09394993 0.0984958  0.0950407\n",
      "  0.10746826 0.09964079 0.10054875 0.10171413]\n",
      " [0.09733234 0.09305285 0.09715999 0.10045511 0.0974492  0.0996761\n",
      "  0.10704806 0.10580473 0.10321229 0.09880934]\n",
      " [0.10171482 0.09109311 0.11402044 0.08883954 0.0983076  0.11333448\n",
      "  0.09481506 0.09778295 0.10352484 0.09656713]\n",
      " [0.08588523 0.09177338 0.09795464 0.11652739 0.10995997 0.09381495\n",
      "  0.10161219 0.09773164 0.10238531 0.10235529]\n",
      " [0.10158101 0.10707733 0.09065122 0.11022    0.10152953 0.09144293\n",
      "  0.10235189 0.10865459 0.09900189 0.08748961]\n",
      " [0.10746091 0.09375875 0.09658296 0.10233415 0.10780829 0.09223858\n",
      "  0.09982894 0.0901238  0.10808448 0.10177913]\n",
      " [0.10072613 0.09810159 0.09083208 0.1089419  0.11432861 0.09718192\n",
      "  0.10246487 0.09871862 0.10022669 0.08847763]\n",
      " [0.09372861 0.08814608 0.09028306 0.09781213 0.10330708 0.08962899\n",
      "  0.10874104 0.11761376 0.09702953 0.11370974]\n",
      " [0.08572881 0.10123974 0.08930653 0.10228685 0.09898794 0.11456291\n",
      "  0.09384816 0.10434137 0.11563104 0.09406669]\n",
      " [0.08931266 0.09320694 0.10315279 0.09209888 0.10560323 0.09540058\n",
      "  0.10458325 0.10421222 0.1048269  0.10760251]\n",
      " [0.10924992 0.0947302  0.10404485 0.0905154  0.10201911 0.09242243\n",
      "  0.1065839  0.10508499 0.09296964 0.10237958]\n",
      " [0.0970998  0.09810896 0.09596613 0.1077304  0.09692458 0.1033263\n",
      "  0.09268964 0.10115994 0.0973298  0.10966439]\n",
      " [0.10484087 0.10399508 0.09484183 0.10006629 0.08342661 0.11844409\n",
      "  0.10563231 0.09905244 0.09144092 0.09825953]\n",
      " [0.10238357 0.09819472 0.09062157 0.1133379  0.0893781  0.09485517\n",
      "  0.10566157 0.10694914 0.09958781 0.0990304 ]\n",
      " [0.09198065 0.09273735 0.08799923 0.10066802 0.11874958 0.09325524\n",
      "  0.09121582 0.10428475 0.10607803 0.11303137]\n",
      " [0.09856603 0.10533162 0.10301414 0.10184284 0.09754442 0.10039327\n",
      "  0.09780694 0.0989386  0.09377048 0.10279158]\n",
      " [0.09356351 0.10998905 0.09031492 0.09481356 0.1066837  0.10605282\n",
      "  0.11147166 0.08713478 0.10288382 0.09709209]\n",
      " [0.09354001 0.10559417 0.10048697 0.09896605 0.10770541 0.10274208\n",
      "  0.10283252 0.08957773 0.09841853 0.10013656]\n",
      " [0.09902113 0.11197031 0.09180532 0.1030078  0.09293386 0.09879848\n",
      "  0.10162076 0.09051778 0.10875877 0.10156579]\n",
      " [0.10203999 0.09542725 0.08745874 0.09597624 0.10084462 0.10170598\n",
      "  0.09971499 0.11152819 0.09116706 0.1141369 ]\n",
      " [0.11314276 0.08328769 0.1012823  0.09411224 0.11207915 0.10631592\n",
      "  0.09566343 0.10222933 0.09578023 0.09610697]\n",
      " [0.09644295 0.10172574 0.0909089  0.09303984 0.0930355  0.10831423\n",
      "  0.10747945 0.09881346 0.10548367 0.10475624]\n",
      " [0.0940092  0.09859417 0.08657219 0.09938594 0.10480413 0.09418479\n",
      "  0.11019436 0.10037541 0.10489377 0.10698607]\n",
      " [0.10159507 0.10119493 0.10605251 0.10065407 0.09754199 0.09830222\n",
      "  0.09763031 0.10016789 0.08847848 0.10838257]\n",
      " [0.10318811 0.10060275 0.09349387 0.09991292 0.09432816 0.09738287\n",
      "  0.11210596 0.09844778 0.10104956 0.0994881 ]\n",
      " [0.11076953 0.09464432 0.09402327 0.09945091 0.10723205 0.10185528\n",
      "  0.09034665 0.09202291 0.10451225 0.10514278]\n",
      " [0.09438113 0.09768215 0.09247018 0.1056068  0.09936862 0.11588474\n",
      "  0.09892388 0.09456541 0.10348805 0.09762908]\n",
      " [0.10064361 0.10255848 0.09994085 0.09320112 0.09696662 0.1032939\n",
      "  0.1036951  0.10017946 0.10169561 0.09782519]\n",
      " [0.0951528  0.09061702 0.09088423 0.10243084 0.10718153 0.10353526\n",
      "  0.10213974 0.10777962 0.09658897 0.10369002]\n",
      " [0.08714811 0.10270362 0.08454996 0.10210266 0.10324632 0.10437147\n",
      "  0.1043457  0.1016226  0.10250301 0.10740656]\n",
      " [0.09328187 0.09034251 0.09230216 0.08476912 0.09868129 0.10909344\n",
      "  0.1116816  0.11056953 0.10341412 0.10586441]\n",
      " [0.11103487 0.10892919 0.09422758 0.09625846 0.08823536 0.10217714\n",
      "  0.10652785 0.10260265 0.08995797 0.10004897]\n",
      " [0.10374489 0.09171812 0.09679437 0.10629106 0.09037528 0.1003414\n",
      "  0.10509261 0.10305855 0.09527712 0.10730663]\n",
      " [0.10278574 0.10026692 0.10287693 0.08745959 0.09607749 0.10525101\n",
      "  0.10120829 0.10441846 0.10933678 0.09031882]\n",
      " [0.09364653 0.10094919 0.0930412  0.09731852 0.09799171 0.09809459\n",
      "  0.09926214 0.11018578 0.10910334 0.10040701]\n",
      " [0.09784048 0.10415176 0.09084404 0.10230416 0.09775596 0.10440128\n",
      "  0.09785086 0.10210423 0.10008062 0.10266662]\n",
      " [0.09736685 0.1144959  0.1030318  0.09411888 0.0995642  0.08642657\n",
      "  0.11191812 0.09088306 0.1118091  0.0903855 ]\n",
      " [0.11424179 0.10573565 0.09765399 0.10343673 0.09904269 0.10164807\n",
      "  0.08947902 0.09815057 0.09012555 0.10048597]\n",
      " [0.10756035 0.09980444 0.09912793 0.09274578 0.10100943 0.10304683\n",
      "  0.10343596 0.08480971 0.09959752 0.1088621 ]\n",
      " [0.10941915 0.10242409 0.08974912 0.11583015 0.10232187 0.11150371\n",
      "  0.08578677 0.0983258  0.08612712 0.09851224]\n",
      " [0.09407642 0.0840957  0.10226238 0.10514107 0.10599253 0.10449585\n",
      "  0.09075125 0.10709395 0.09719645 0.10889436]\n",
      " [0.09968642 0.09536628 0.0821675  0.09575617 0.10343701 0.10463099\n",
      "  0.10534325 0.09577507 0.10210465 0.1157327 ]\n",
      " [0.09219203 0.10772511 0.09436738 0.10870086 0.09705126 0.10028584\n",
      "  0.10463109 0.09831719 0.09688429 0.09984493]\n",
      " [0.09308218 0.10018568 0.09203377 0.1135302  0.10268293 0.10029814\n",
      "  0.10036061 0.08958525 0.10018173 0.10805949]\n",
      " [0.09742288 0.09532675 0.0898606  0.10222552 0.1129796  0.09568271\n",
      "  0.0966723  0.11154367 0.09536802 0.10291789]\n",
      " [0.1018258  0.09663912 0.08394434 0.10455222 0.09080919 0.09833522\n",
      "  0.10102729 0.10139764 0.10785492 0.11361428]\n",
      " [0.10553622 0.09445099 0.09849373 0.11117432 0.09227163 0.10549513\n",
      "  0.10883491 0.10844748 0.0843542  0.09094141]\n",
      " [0.09771009 0.09922606 0.09320156 0.10291767 0.0989757  0.11243526\n",
      "  0.10121983 0.09388889 0.08713832 0.11328666]\n",
      " [0.09868616 0.09228016 0.09915479 0.10323097 0.09675331 0.09357955\n",
      "  0.09442322 0.10848306 0.09903365 0.11437512]\n",
      " [0.10480431 0.10471864 0.08784475 0.09931184 0.09363283 0.09745289\n",
      "  0.11005216 0.10328513 0.09448183 0.10441557]\n",
      " [0.10592088 0.10408402 0.08785716 0.10773503 0.10384083 0.09587417\n",
      "  0.09424478 0.10019423 0.09916244 0.10108653]\n",
      " [0.0951794  0.100862   0.09024166 0.10326703 0.09256552 0.1177162\n",
      "  0.10005259 0.09013399 0.10138338 0.10859825]\n",
      " [0.11794154 0.09375381 0.09057853 0.0852889  0.10379738 0.1046546\n",
      "  0.1072841  0.09630646 0.09423702 0.10615773]\n",
      " [0.09246079 0.09252689 0.09540966 0.10194773 0.10195704 0.10286041\n",
      "  0.10200024 0.10718554 0.10626713 0.09738452]\n",
      " [0.09246605 0.11738533 0.08941676 0.08635323 0.09584735 0.10491171\n",
      "  0.1088025  0.10131519 0.10271379 0.10078808]\n",
      " [0.11135818 0.08764508 0.09036168 0.1059169  0.10365286 0.09921711\n",
      "  0.10790201 0.10706352 0.09410169 0.09278101]\n",
      " [0.0976218  0.0988491  0.09864017 0.10820314 0.09332157 0.0939163\n",
      "  0.11450057 0.10231069 0.10182483 0.09081177]\n",
      " [0.10739539 0.08732242 0.088517   0.09911837 0.10676527 0.1065166\n",
      "  0.10737637 0.100361   0.10237963 0.09424791]]\n",
      "INFO:tensorflow:loss = 2.2688186, step = 201\n",
      "INFO:tensorflow:probabilities = [[0.0983694  0.09616239 0.08873629 0.10010988 0.08998745 0.11572383\n",
      "  0.10123408 0.10169923 0.10495976 0.10301767]\n",
      " [0.09228286 0.09630745 0.09560168 0.10047525 0.10792827 0.09520219\n",
      "  0.09542559 0.0983183  0.10590933 0.11254909]\n",
      " [0.10127487 0.09648947 0.09868207 0.08888634 0.09854338 0.10675324\n",
      "  0.11998037 0.09239329 0.10657243 0.09042463]\n",
      " [0.09710257 0.09546504 0.09104542 0.0946617  0.10934571 0.10328138\n",
      "  0.09623855 0.11271347 0.10555854 0.09458767]\n",
      " [0.09491564 0.09937075 0.07969137 0.1000345  0.1104264  0.09760527\n",
      "  0.11692932 0.09961537 0.10771851 0.09369286]\n",
      " [0.09896701 0.0936196  0.09231239 0.10683139 0.10501003 0.10644115\n",
      "  0.08954823 0.10192638 0.10142935 0.10391439]\n",
      " [0.09322533 0.09362105 0.08012778 0.10691076 0.10164876 0.09813028\n",
      "  0.11211055 0.1025136  0.10051664 0.11119525]\n",
      " [0.10296386 0.09179574 0.10478099 0.10120583 0.09633403 0.10382982\n",
      "  0.09019523 0.09572712 0.09880158 0.11436576]\n",
      " [0.09433179 0.08700655 0.09947452 0.09853052 0.10512451 0.11114567\n",
      "  0.10413983 0.09865012 0.10001213 0.1015844 ]\n",
      " [0.09341251 0.10869697 0.09808976 0.10075629 0.09608348 0.08823878\n",
      "  0.10480244 0.10090432 0.1041204  0.10489515]\n",
      " [0.11086383 0.08794471 0.10844148 0.09170106 0.09777048 0.10452586\n",
      "  0.10604478 0.09772369 0.09595408 0.09903005]\n",
      " [0.10018009 0.10696575 0.08596222 0.09343847 0.09720264 0.08633912\n",
      "  0.10347586 0.11651147 0.10437337 0.10555094]\n",
      " [0.09711064 0.10483998 0.0796254  0.1017841  0.10025608 0.10763832\n",
      "  0.109115   0.10078679 0.09206353 0.10678018]\n",
      " [0.0990985  0.09314692 0.09341595 0.09509799 0.10171895 0.10789925\n",
      "  0.09757204 0.10589741 0.10489596 0.10125706]\n",
      " [0.0964193  0.09177913 0.0969051  0.0902953  0.09275266 0.09617709\n",
      "  0.11804096 0.1159469  0.10656318 0.09512041]\n",
      " [0.10256609 0.09565046 0.08443581 0.09963312 0.10219704 0.10268222\n",
      "  0.09889233 0.09666166 0.11570618 0.10157506]\n",
      " [0.10105287 0.09392492 0.08702604 0.10377657 0.10526691 0.09646802\n",
      "  0.11160388 0.10384575 0.09283537 0.10419964]\n",
      " [0.09087871 0.1064026  0.09083264 0.10077677 0.10203526 0.10016312\n",
      "  0.1049826  0.09709643 0.10123217 0.10559972]\n",
      " [0.09323512 0.09361107 0.08664275 0.09652546 0.09423566 0.10704022\n",
      "  0.10541461 0.11360382 0.09701663 0.11267468]\n",
      " [0.09587103 0.10699239 0.08626494 0.09854392 0.09278036 0.11092231\n",
      "  0.10438596 0.10059018 0.10054207 0.10310691]\n",
      " [0.08979131 0.09495374 0.08791024 0.09915712 0.09587621 0.10412016\n",
      "  0.10100552 0.10838831 0.10183353 0.11696388]\n",
      " [0.11150221 0.09615207 0.08882391 0.09906638 0.09899999 0.09123943\n",
      "  0.10328247 0.10831735 0.09284542 0.10977079]\n",
      " [0.12145831 0.08856334 0.09722632 0.09366468 0.0974343  0.09655429\n",
      "  0.09847306 0.11123759 0.09594914 0.09943897]\n",
      " [0.09516586 0.10025066 0.09441935 0.09061275 0.09971932 0.09962374\n",
      "  0.10867985 0.10231122 0.103769   0.10544819]\n",
      " [0.0860876  0.09863468 0.0837728  0.10395471 0.10658783 0.10144637\n",
      "  0.10199056 0.09276975 0.11350587 0.11124979]\n",
      " [0.09927859 0.10741872 0.10749286 0.08942321 0.10073343 0.10161798\n",
      "  0.10900606 0.09293558 0.09965505 0.0924385 ]\n",
      " [0.10047222 0.08614632 0.09594282 0.10494528 0.10371641 0.10505556\n",
      "  0.10365045 0.08855376 0.10076328 0.11075391]\n",
      " [0.0878271  0.09281331 0.09738966 0.10361307 0.10215967 0.10732334\n",
      "  0.10339565 0.10096148 0.09381455 0.1107021 ]\n",
      " [0.10033224 0.10167651 0.09157831 0.10304911 0.09699597 0.09984347\n",
      "  0.10833625 0.09898049 0.09789222 0.10131534]\n",
      " [0.10470992 0.09931032 0.10488887 0.09863794 0.1025771  0.10656976\n",
      "  0.10145973 0.09083717 0.09461562 0.09639359]\n",
      " [0.10649349 0.1003874  0.09306218 0.09859411 0.10199795 0.10732792\n",
      "  0.09895423 0.10788382 0.08656419 0.09873468]\n",
      " [0.10322365 0.10040157 0.08721801 0.10473996 0.10365055 0.10625303\n",
      "  0.10801616 0.08754876 0.09880361 0.1001446 ]\n",
      " [0.10317998 0.09945528 0.09479117 0.09842821 0.09446216 0.11962824\n",
      "  0.10336147 0.09104061 0.0975629  0.09808999]\n",
      " [0.08763889 0.08652686 0.10010592 0.09090324 0.1172927  0.09554687\n",
      "  0.09379914 0.10278641 0.1004829  0.12491701]\n",
      " [0.09655554 0.10336567 0.08747854 0.09997228 0.09962022 0.11183389\n",
      "  0.10349104 0.09707335 0.10004526 0.10056418]\n",
      " [0.08569087 0.10339283 0.10555112 0.10086789 0.10114954 0.0987898\n",
      "  0.10298469 0.10402778 0.09679299 0.10075246]\n",
      " [0.10623525 0.09773194 0.1119711  0.10175829 0.08735973 0.10105105\n",
      "  0.09974358 0.09830614 0.09492808 0.10091489]\n",
      " [0.11351537 0.09383399 0.0895884  0.10531524 0.09982478 0.09940028\n",
      "  0.10593835 0.10608732 0.09508138 0.09141485]\n",
      " [0.09385726 0.10545658 0.0929601  0.0960005  0.09770486 0.10917002\n",
      "  0.10099862 0.10154189 0.09851044 0.1037997 ]\n",
      " [0.09690308 0.0979101  0.09261709 0.11449157 0.10152288 0.10376836\n",
      "  0.09132836 0.09828656 0.09640024 0.10677179]\n",
      " [0.10425717 0.09062516 0.08961515 0.11088973 0.09906012 0.09720178\n",
      "  0.10387485 0.10626248 0.09428697 0.10392662]\n",
      " [0.09914543 0.10694627 0.09146423 0.08804678 0.09652986 0.10682423\n",
      "  0.10304238 0.09138559 0.10639726 0.11021794]\n",
      " [0.10336571 0.09090731 0.08555669 0.09655246 0.08987608 0.08894845\n",
      "  0.09606422 0.12612642 0.1068046  0.115798  ]\n",
      " [0.0891262  0.09572544 0.09159019 0.09449702 0.10084146 0.09628938\n",
      "  0.10102302 0.12023895 0.10519399 0.10547432]\n",
      " [0.1043498  0.08809836 0.09306525 0.10407086 0.10001263 0.11889438\n",
      "  0.09157685 0.09251931 0.09434468 0.11306785]\n",
      " [0.09937064 0.10220825 0.0792739  0.10850993 0.10680728 0.11937449\n",
      "  0.09885605 0.09751625 0.09513782 0.0929454 ]\n",
      " [0.10310918 0.0862391  0.09261993 0.10549831 0.11017568 0.10427482\n",
      "  0.10453668 0.09549772 0.10051206 0.09753656]\n",
      " [0.1009227  0.10520263 0.08938351 0.09414253 0.10106888 0.09750529\n",
      "  0.09803762 0.1082893  0.10536395 0.10008362]\n",
      " [0.09302419 0.09548975 0.10117979 0.10300659 0.10469848 0.10418064\n",
      "  0.10542972 0.09528598 0.09894383 0.09876108]\n",
      " [0.09930717 0.09440046 0.08600122 0.10052859 0.10813027 0.09612215\n",
      "  0.10336656 0.0986199  0.09694868 0.11657497]\n",
      " [0.10903826 0.09534675 0.10113333 0.10267529 0.08794967 0.10558625\n",
      "  0.10251046 0.09431124 0.10202462 0.09942406]\n",
      " [0.09760547 0.09403124 0.09367266 0.10555011 0.09672748 0.10758142\n",
      "  0.09323838 0.10097282 0.10276319 0.10785722]\n",
      " [0.11571315 0.08801716 0.10697565 0.09121339 0.09401021 0.0960687\n",
      "  0.1210911  0.09848995 0.09373152 0.09468921]\n",
      " [0.10486823 0.09669736 0.08421823 0.09786265 0.10056268 0.09630831\n",
      "  0.10100409 0.10964609 0.10017367 0.1086587 ]\n",
      " [0.09675835 0.09607429 0.09235687 0.09810591 0.09514716 0.11529928\n",
      "  0.10441596 0.09563478 0.10355153 0.10265584]\n",
      " [0.08961854 0.09596531 0.10878662 0.09946365 0.10832433 0.08982026\n",
      "  0.10079217 0.09731908 0.1133244  0.09658564]\n",
      " [0.10669124 0.09540006 0.09432183 0.09789    0.09839129 0.10103469\n",
      "  0.09966731 0.09364638 0.10518366 0.10777348]\n",
      " [0.09879183 0.09610692 0.09716767 0.11073862 0.10089087 0.09460074\n",
      "  0.09662057 0.1063328  0.09510395 0.10364601]\n",
      " [0.09555555 0.11040092 0.08908286 0.10169905 0.09412282 0.09970615\n",
      "  0.10455    0.10022638 0.10013767 0.1045186 ]\n",
      " [0.09891406 0.11244731 0.09050895 0.09815798 0.09759806 0.1078371\n",
      "  0.09522471 0.09499696 0.0936167  0.11069821]\n",
      " [0.09867022 0.09479783 0.08457184 0.10546932 0.10518762 0.09326294\n",
      "  0.10067116 0.09867018 0.11180329 0.10689566]\n",
      " [0.09443168 0.10089834 0.10886706 0.09957022 0.09837737 0.09163344\n",
      "  0.1014505  0.10533952 0.10103095 0.09840091]\n",
      " [0.09344051 0.08921278 0.09400385 0.10948709 0.09960113 0.09827387\n",
      "  0.11592545 0.09932882 0.09097408 0.10975234]\n",
      " [0.11687994 0.09821309 0.09162253 0.09879656 0.09465031 0.10254776\n",
      "  0.10544772 0.08883018 0.10722544 0.09578647]\n",
      " [0.08820694 0.10146514 0.10464052 0.10158937 0.10221531 0.09919494\n",
      "  0.10598884 0.09961802 0.09505725 0.10202368]\n",
      " [0.08918765 0.099783   0.09291396 0.09862433 0.09006118 0.11119826\n",
      "  0.11188636 0.09843466 0.10836435 0.09954628]\n",
      " [0.11083221 0.09629391 0.10128801 0.10439184 0.10146277 0.09708258\n",
      "  0.09988786 0.08789167 0.09450312 0.10636595]\n",
      " [0.10417656 0.09750325 0.09671246 0.10633346 0.09271374 0.09734719\n",
      "  0.10801158 0.10046875 0.09056397 0.10616899]\n",
      " [0.09869314 0.11492562 0.09580971 0.09171386 0.09416291 0.10483941\n",
      "  0.09840998 0.10266585 0.10029145 0.09848805]\n",
      " [0.10007375 0.0865255  0.0956138  0.10267343 0.10277318 0.09700199\n",
      "  0.10447832 0.10080162 0.09740317 0.11265524]\n",
      " [0.09183289 0.10041941 0.10223786 0.09402815 0.10588158 0.09730744\n",
      "  0.11604392 0.09220045 0.09825477 0.10179354]\n",
      " [0.09330881 0.09495895 0.09118475 0.09946945 0.10262374 0.10814407\n",
      "  0.09501933 0.11246253 0.10044079 0.10238755]\n",
      " [0.08453339 0.08152885 0.09064116 0.09217027 0.1054907  0.10854449\n",
      "  0.11388224 0.11006541 0.09780725 0.11533614]\n",
      " [0.1000599  0.10263279 0.09309781 0.11477827 0.1076071  0.09611868\n",
      "  0.09371278 0.08932987 0.10159683 0.10106595]\n",
      " [0.0960073  0.10742633 0.09022889 0.09901342 0.09968911 0.10984544\n",
      "  0.0939227  0.10214842 0.10000818 0.10171016]\n",
      " [0.09069847 0.0931365  0.09769288 0.09508912 0.10519773 0.09385373\n",
      "  0.10529789 0.1083464  0.10433689 0.10635041]\n",
      " [0.11749658 0.10011144 0.09654841 0.1155447  0.10305845 0.08978165\n",
      "  0.08727229 0.09950393 0.09985555 0.09082702]\n",
      " [0.09513979 0.10970084 0.09465703 0.1020906  0.08850399 0.1175311\n",
      "  0.10671563 0.1038572  0.08898215 0.0928216 ]\n",
      " [0.09234063 0.10804965 0.0886617  0.09875533 0.09521724 0.11386365\n",
      "  0.10516522 0.10355175 0.09150703 0.10288784]\n",
      " [0.09505707 0.09154502 0.09573073 0.09013229 0.10073382 0.11267526\n",
      "  0.10057471 0.10720972 0.10441441 0.10192703]\n",
      " [0.09312783 0.0967078  0.07962703 0.09182575 0.10521282 0.11217599\n",
      "  0.10236199 0.10455879 0.09722564 0.11717635]\n",
      " [0.10759378 0.09199513 0.1048377  0.09715499 0.09292112 0.09075341\n",
      "  0.10472538 0.08894556 0.11364292 0.10742993]\n",
      " [0.09585698 0.10306396 0.09445945 0.10089724 0.0877087  0.10986921\n",
      "  0.11371171 0.09606239 0.09436426 0.10400609]\n",
      " [0.09718898 0.08834802 0.09628598 0.10538733 0.09752095 0.10814902\n",
      "  0.09664374 0.10179034 0.10835423 0.10033147]\n",
      " [0.1013535  0.09616568 0.08570322 0.09705827 0.10640495 0.09842261\n",
      "  0.1077145  0.10337028 0.09707675 0.10673016]\n",
      " [0.09575597 0.09560503 0.0923389  0.09351443 0.09762429 0.10543677\n",
      "  0.10102207 0.11317381 0.09475788 0.11077081]\n",
      " [0.09284335 0.10003794 0.09969015 0.10520146 0.0855263  0.10842963\n",
      "  0.11026376 0.10413722 0.09644776 0.09742244]\n",
      " [0.09516986 0.1054199  0.09369131 0.10050914 0.10726013 0.10216729\n",
      "  0.09637213 0.09807767 0.10335947 0.0979731 ]\n",
      " [0.09357605 0.09723345 0.09368183 0.10560246 0.11046246 0.09647068\n",
      "  0.1027777  0.10362087 0.09825029 0.09832418]\n",
      " [0.09381831 0.09080208 0.08092246 0.10032063 0.10490771 0.10385761\n",
      "  0.10351422 0.11667843 0.1050748  0.10010378]\n",
      " [0.08758762 0.09556945 0.09993673 0.09269844 0.10909579 0.1003416\n",
      "  0.10981137 0.10377082 0.10064184 0.10054638]\n",
      " [0.10008806 0.09594214 0.10705055 0.09296962 0.08658215 0.1068415\n",
      "  0.11307155 0.10090707 0.09489247 0.10165491]\n",
      " [0.09696601 0.10177535 0.0880862  0.09759309 0.10269626 0.11583088\n",
      "  0.10514468 0.09567156 0.10191645 0.09431962]\n",
      " [0.09849088 0.091483   0.10560266 0.09182633 0.11455072 0.09382948\n",
      "  0.10866596 0.10134451 0.09786148 0.09634506]\n",
      " [0.10548569 0.10309645 0.08771377 0.10607558 0.0956303  0.10075455\n",
      "  0.09368323 0.1119042  0.10357051 0.09208574]\n",
      " [0.10397825 0.09727312 0.08784629 0.09680326 0.11376061 0.08862637\n",
      "  0.09021665 0.10506825 0.1020982  0.11432903]\n",
      " [0.11105072 0.09139615 0.09057065 0.10309483 0.10040143 0.1040043\n",
      "  0.09682999 0.10131667 0.09629349 0.10504179]\n",
      " [0.09470578 0.09678881 0.08891893 0.09944886 0.1057273  0.10311065\n",
      "  0.1032854  0.11170096 0.09630495 0.10000845]\n",
      " [0.10081006 0.10670745 0.08490656 0.10327941 0.10190038 0.10917062\n",
      "  0.09479307 0.09395425 0.10313297 0.1013452 ]\n",
      " [0.09050635 0.1046422  0.09316538 0.09904566 0.09663504 0.09957866\n",
      "  0.11321272 0.09571711 0.1023234  0.10517354]] (6.406 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-45fb2a2c88fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-45fb2a2c88fc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m       \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       hooks=[logging_hook])\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;31m# Evaluate the model and print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    544\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1023\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vbhalala/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"Convolutional Neural Network Estimator for MNIST, built with tf.layers.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel(in this case black)\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "  # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x5 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #################################\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 10]\n",
    "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  # Load training and eval data\n",
    "  mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "  train_data = mnist.train.images  # Returns np.array\n",
    "  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "  eval_data = mnist.test.images  # Returns np.array\n",
    "  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "  # Create the Estimator\n",
    "  mnist_classifier = tf.estimator.Estimator(\n",
    "      model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n",
    "  # Set up logging for predictions\n",
    "  # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "  logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "  # Train the model\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=100,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "  mnist_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=500,\n",
    "      hooks=[logging_hook])\n",
    "\n",
    "  # Evaluate the model and print results\n",
    "  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": eval_data},\n",
    "      y=eval_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "  print(eval_results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
